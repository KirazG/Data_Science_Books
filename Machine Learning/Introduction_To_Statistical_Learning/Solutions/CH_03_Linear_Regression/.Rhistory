setwd("~/GitHub/Data_Science_Books/Machine Learning/Introduction_To_Statistical_Learning/Solutions/CH_03_Linear_Regression")
rm(list = ls(all.names = TRUE))
require(ISLR)
require(MASS)
require(corrplot)
fix(Boston)
corrplot(corr = cor(Boston[,1:12]))
?Boston
corrplot(corr = cor(Boston))
data("Boston")
dfBoston = Boston
fix(dfBoston)
str(dfBoston)
summary(dfBoston)
names(dfBoston)
lm1 = lm(medv ~ ., data = dfBoston)
lm1
print(llm1)
print(lm1)
summary(lm1)
nrow(dfBoston)
492-13-1
506-13-1
summary(lm1)
plot(lm1)
hist(lm1$residuals)
hist(lm1$residuals, breaks = 20)
mean(lm1$residuals)
nrow(dfBoston)
names(lm1)
lm1$coefficients
lm1$residuals
lm1$effects
lm1$rank
lm1$fitted.values
lm1$assign
lm1$qr
lm1$df.residual
lm1$xlevels
lm1$call
lm1$terms
lm1$model
lm1$model
summary(lm1)
coef(lm1)
coefficients(lm1)
confint(lm1)
round(confint(lm1), 2)
summary(lm1)
lm2 = lm(medv ~ lstat, data = dfBoston)
summary(lm2)
plot(lm2)
plot(lstat, medv)
plot(dfBoston$lstat, dfBoston$medv)
abline(lm2)
corr(dfBoston$lstat, dfBoston$medv)
cor(dfBoston$lstat, dfBoston$medv)
abline(lm2, lwd=2)
abline(lm2, lwd=5)
abline(lm2, lwd=2)
abline(lm2, lwd=1.5)
abline(lm2, lwd=1)
plot(dfBoston$lstat, dfBoston$medv)
abline(lm2, lwd=0.5)
abline(lm2, lwd=1)
abline(lm2, lwd=2)
abline(lm2, lwd=2, col = "blue")
plot(lstat ,medv ,col ="red ")
plot(dfBoston$lstat ,dfBoston$medv ,col ="red ")
plot(dfBoston$lstat ,dfBoston$medv ,col ="red ", pch = 15)
plot(dfBoston$lstat ,dfBoston$medv ,col ="red ", pch = 10)
plot(dfBoston$lstat ,dfBoston$medv ,col ="red ", pch = 1)
plot(dfBoston$lstat ,dfBoston$medv ,col ="red ", pch = 20)
plot(predict (lm2), residuals (lm2))
hatvalues(lm2)
H <- hatvalues(lm2)
H
names(H)
sort(H)
sort(round(H,2))
sort(round(H,3))
sort(round(H,4))
sort(round(H,4), decreasing = TRUE)
plot(hatvalues(lm2))
### Load and Copy Boston Housing data
data("Boston")
dfBoston = Boston
### Take a look at the data
fix(dfBoston)
str(dfBoston)
summary(dfBoston)
### Look at variable names in the set
names(dfBoston)
### Look at variable names in the set
lm.fit =lm(medv ~ .,data = dfBoston)
summary (lm.fit)
?summary.lm
S <- summary(lm2)
S$call
S$terms
S$residuals
S$aliased
S$sigma
S$df
S$r.squared
S$adj.r.squared
S$cov.unscaled
S <- summary(lm.fit)
S$cov.unscaled
S$fstatistic
S$df
require(car)
objSum <- summary(lm.fit)
S$df
objSum$fstatistic
summary(lm.fit)
objSum$residuals
objSum$sigma
summary(lm.fit)
objSum$sigma
# R-square & Adjusted-R-square
objSum$r.squared
objSum$adj.r.squared
summary(dfBoston)
### Look at variable names in the set
names(dfBoston)
### Look at variable names in the set
lm.fit =lm(medv ~ .,data = dfBoston)
summary (lm.fit)
### To access individual elements of Sumamry
objSum <- summary(lm.fit)
# Model's F-statistics
objSum$fstatistic
?update
lm.fit1 = update(object = lm.fit, formula. = medv ~ .-age)
lm.fit
lm.fit1
lm.fit1 = update(object = lm.fit, formula. = medv ~ .-age -nox)
lm.fit1
summary(lm.fit1)
lm.fit2 = update(object = lm.fit1, formula. = medv ~ .-age-indus)
summary(lm.fit1)
summary(lm.fit2)
# Exlcude indus as it's not significant
lm.fit2 = update(object = lm.fit, formula. = medv ~ .-age-indus)
summary(lm.fit2)
# Exlcude indus as it's not significant
lm.fit2 = update(object = lm.fit1, formula. = medv ~ .-indus)
summary(lm.fit2)
# Exlcude indus as it's not significant
lm.fit2 = update(object = lm.fit1, formula. = medv ~ .-indus)
summary(lm.fit2)
hist(dfBoston$medv)
hist(dfBoston$medv, breaks = 50)
hist(dfBoston$medv)
hist(dfBoston$medv, breaks = 50)
hist(dfBoston$medv, breaks = 100)
abline(v = mean(dfBoston$medv))
hist(log(dfBoston$medv), breaks = 50)
abline(v = mean(log(dfBoston$medv)))
sd(log(dfBoston$medv))
?I
lm1 = lm(formula = medv ~ lstat, data = dfBoston)
lm1 = lm(formula = medv ~ lstat, data = dfBoston)
summary(lm1)
# I() is required in the forula to preserve the meaning of ^ as an arithmatic operator
lm2 = lm(formula = medv ~ lstat + I(lstat ^ 2), data = dfBoston)
summary(lm1)
lm1 = lm(formula = medv ~ lstat, data = dfBoston)
summary(lm1)
# I() is required in the forula to preserve the meaning of ^ as an arithmatic operator
lm2 = lm(formula = medv ~ lstat + I(lstat^2), data = dfBoston)
summary(lm2)
# Predictor lstat with power 3
lm3 = lm(formula = medv ~ lstat + I(lstat^3), data = dfBoston)
summary(lm3)
anova(lm1, lm2)
anova(lm1, lm2, lm3)
# Using ANOVA - Compare lm1 & lm2
anova(lm1, lm2)
anova(lm2, lm3)
anova(lm1, lm3)
anova(lm1, lm2, lm3)
?Contrasts
?contrasts
contrasts(dfBoston$chas)
contrasts(as.factor(dfBoston$chas))
contrasts(as.factor(dfBoston$medv))
##### Cleanup environment & Set working directory
rm(list = ls(all.names = TRUE))
require(ISLR)
require(MASS)
require(corrplot)
require(car)
a
dfAuto = data("Auto")
dfAuto = data(Auto)
dfAuto
dfAuto = Auto
str(Auto)
lmSLR = lm(mpg ~ horsepower, data = dfAuto)
summary(lmSLR)
mean(lmSLR$fitted.values)
predict(fit, data.frame(horsepower = 98), interval = "confidence")
predict(lmSLR, data.frame(horsepower = 98), interval = "confidence")
predict(lmSLR, data.frame(horsepower = 98), interval = "prediction")
plot(x = dfAuto$mpg, y = dfAuto$horsepower)
plot(x = dfAuto$mpg, y = dfAuto$horsepower, col = "blue")
plot(x = dfAuto$mpg, y = dfAuto$horsepower, col = "blue", xlab = "MPG", ylab = "Horsepower")
abline(reg = lmSLR)
plot(x = dfAuto$mpg, y = dfAuto$horsepower)
plot(x = dfAuto$mpg, y = dfAuto$horsepower, col = "blue", xlab = "MPG", ylab = "Horsepower")
abline(reg = lmSLR)
plot(x = dfAuto$horsepower, y = dfAuto$mpg)
plot(x = dfAuto$horsepower, y = dfAuto$mpg, col = "blue", xlab = "MPG", ylab = "Horsepower")
plot(x = dfAuto$horsepower, y = dfAuto$mpg)
plot(x = dfAuto$horsepower, y = dfAuto$mpg, col = "blue", ylab = "MPG", xlab = "Horsepower")
abline(reg = lmSLR)
abline(reg = lmSLR, col = "red")
par(mfrow = c(2,2))
par(mfrow = c(2,2))
par(mfrow = c(2,2))
plot(lmSLR)
par(mfrow = c(1,1))
str(dfAuto)
plot(dfAuto[,1:(ncol(dfAuto)-1)])
pairs(dfAuto)
pairs(dfAuto[,1:(ncol(dfAuto)-1)], col = "blue")
# Scatterplot 1-excluding name column
str(dfAuto)
plot(dfAuto[,1:(ncol(dfAuto)-1)], col = "red")
# Scatterplot 2-excluding name column
pairs(dfAuto[,1:(ncol(dfAuto)-1)], col = "blue")
CORR = cor(dfAuto[,1:(ncol(dfAuto)-1)])
source('~/GitHub/Data_Science_Books/Machine Learning/Introduction_To_Statistical_Learning/Solutions/CH_03_Linear_Regression/CH_03_Linear_Regression_APPLIED.R', echo=TRUE)
corrplot(corr = CORR)
source('~/GitHub/Data_Science_Books/Machine Learning/Introduction_To_Statistical_Learning/Solutions/CH_03_Linear_Regression/CH_03_Linear_Regression_APPLIED.R')
names(dfAuto)
summary(lmMLR)
lmMLR = lm(formula = mpg ~ .-name, data = dfAuto)
summary(lmMLR)
par(mfrow = c(2,2))
plot(lmMLR)
par(mfrow = c(2,2))
#################################################################################
##### BOOK: INTRODUCTION TO STATISTICAL LEARNING                            #####
##### CHAPTER: 03 - LINEAR REGRESSION                                       #####
##### APPLIED EXERCISE CODE                                                 #####
#################################################################################
##### Cleanup environment & Set working directory
rm(list = ls(all.names = TRUE))
require(ISLR)
require(MASS)
require(corrplot)
require(car)
#################################################################################
##### EXERCISE 08:                                                          #####
#################################################################################
### This question involves the use of simple linear regression on the Auto data set.
### (a) Use the lm() function to perform a simple linear regression with
### mpg as the response and horsepower as the predictor. Use the summary() function
### to print the results. Comment on the output.
dfAuto = Auto
str(Auto)
lmSLR = lm(mpg ~ horsepower, data = dfAuto)
summary(lmSLR)
### i. Is there a relationship between the predictor and the response?
# YES - There is a strong relationship
# The overall F-statistic for model is 599 and p-value is 2.2 X 10e-16 which is ~0.
# The coefficient of horsepower is also significnat. Hence there is strong relationship
# between the predictor and the response
### ii. How strong is the relationship between the predictor and the response?
# RSE for the model is 4.906 while the mean of fitted.values (or response) is 23.446
# That gives % error as = 4.906*100/23.446 = 20.93%
# Also note that Adjusted R-Square = 0.6049 OR horsepower is able to explain 60.49%
# of variation in milage (mpg)
### iii. Is the relationship between the predictor and the response positive or negative?
# Negative: as the coefficient of horsepower is negative - which is intuitive too !
### iv. What is the predicted mpg associated with a horsepower of 98? What are the associated
### 95% confidence and prediction intervals?
# Y = 39.9359 - 0.1578 * horsepower
# Confidence Interval
predict(lmSLR, data.frame(horsepower = 98), interval = "confidence")
# Prediciton Interval
predict(lmSLR, data.frame(horsepower = 98), interval = "prediction")
### (b) Plot the response and the predictor. Use the abline() function to display the least squares regression line.
plot(x = dfAuto$horsepower, y = dfAuto$mpg)
plot(x = dfAuto$horsepower, y = dfAuto$mpg, col = "blue", ylab = "MPG", xlab = "Horsepower")
abline(reg = lmSLR, col = "red")
### (c) Use the plot() function to produce diagnostic plots of the least squares regression fit.
### Comment on any problems you see with the fit.
par(mfrow = c(2,2))
plot(lmSLR)
par(mfrow = c(1,1))
# OBSERVATION:
# Residual Vs Fitted plot displays a V shaped patterns & indicates non-linearity
# in the data. Also plot of Studentized Residual Vs. Leverage indicates presence of # few High Leverage data point and few outliers.
#################################################################################
##### EXERCISE 09:                                                          #####
#################################################################################
### This question involves the use of multiple linear regression on the Auto data set.
### (a) Produce a scatterplot matrix which includes all of the variables in the data set.
# Scatterplot 1-excluding name column
str(dfAuto)
plot(dfAuto[,1:(ncol(dfAuto)-1)], col = "red")
# Scatterplot 2-excluding name column
pairs(dfAuto[,1:(ncol(dfAuto)-1)], col = "blue")
### (b) Compute the matrix of correlations between the variables using the function cor().
### You will need to exclude the name variable, cor() which is qualitative.
# Correlation calculation
CORR = cor(dfAuto[,1:(ncol(dfAuto)-1)])
CORR
# Correlogram
corrplot(corr = CORR)
### Use the lm() function to perform a multiple linear regression with mpg as the response
### and all other variables except name as the predictors. Use the summary() function to print ### the results. Comment on the output. For instance:
lmMLR = lm(formula = mpg ~ .-name, data = dfAuto)
summary(lmMLR)
## i. Is there a relationship between the predictors and the response?
# YES - the overall model is significant based on F-Statistic and p-Value.
## ii. Which predictors appear to have a statistically significant relationship to the response?
# displacement, weight, year, origin
## What does the coefficient for the year variable suggest?
# Coefficient of year variable has positive sign and a value of 0.7508 which indicates that -
# when value of year increases by 1 - keeping all other variable constant -  the AVERAGE milage value # increases by 0.7508 (OR AVERAGE effect of year on milage increases by 0.7508)
# Intuitively, the cars become more fuel-efficient year-on-year.
# (d) Use the plot() function to produce diagnostic plots of the linear regression fit.
# Comment on any problems you see with the fit. Do the residual plots suggest any unusually
# large outliers? Does the leverage plot identify any observations with unusually high
# leverage?
names(dfAuto)
str(dfAuto)
summary(dfAuto)
names(dfAuto)
lmTemp = lm(mpg ~ weight*horsepower, data = dfAuto)
summary(lmTemp)
names(dfAuto)
lmInteraction = lm(formula = mpg ~ cylinder*displacement + horsepower*weight + acceleration*weight, data = dfAuto)
lmInteraction = lm(formula = mpg ~ cylinders*displacement + horsepower*weight + acceleration*weight, data = dfAuto)
summary(lmInteraction)
displacement*weight, data = dfAuto)
cylinders*horsepower + cylinder*acceleration + horsepower*displacement + displacement*weight, data = dfAuto)
lmInteraction = lm(formula = mpg ~ cylinders*displacement + horsepower*weight + acceleration*weight + cylinders*horsepower + cylinder*acceleration + horsepower*displacement + displacement*weight, data = dfAut)
lmInteraction = lm(formula = mpg ~ cylinders*displacement + horsepower*weight + acceleration*weight + cylinders*horsepower + cylinder*acceleration + horsepower*displacement + displacement*weight, data = dfAuto)
lmInteraction = lm(formula = mpg ~ cylinders*displacement + horsepower*weight + acceleration*weight + cylinders*horsepower + cylinders*acceleration + horsepower*displacement + displacement*weight, data = dfAuto)
summary(lmInteraction)
# Correlation calculation
CORR = cor(dfAuto[,1:(ncol(dfAuto)-1)])
CORR
lmInteraction = lm(formula = mpg ~ cylinders*displacement + horsepower*displacement + displacement*weight, data = dfAuto)
summary(lmInteraction)
hist(mpg)
hist(dfAuto$mpg, breaks = 100)
MPG = sort(dfAuto$mpg)
MPG
hist(MPG)
hist(MPG, breaks = 100)
plot(dfAuto$mpg)
plot(sort(dfAuto$mpg))
plot(sort(dfAuto$horsepower))
plot((dfAuto$horsepower))
plot(sort(dfAuto$horsepower))
plot(sqrt(sort(dfAuto$horsepower)))
plot(sort(dfAuto$horsepower))
plot(sqrt(sort(dfAuto$horsepower)))
plot(log(dfAuto$horsepower))
plot(log(sort(dfAuto$horsepower))
)
plot((dfAuto$horsepower))
plot(sort(dfAuto$horsepower))
plot(log(dfAuto$horsepower))
plot(logsort((dfAuto$horsepower)))
plot(log(sort((dfAuto$horsepower))))
plot(dfAuto$cylinders)
names(dfAuto)
plot(dfAuto$displacement)
plot(sort(dfAuto$cylinders))
plot(sort(dfAuto$displacement))
plot(sort(dfAuto$horsepower))
par(mfrow = c(2,2))
# "cylinders" is a semi-discrete variable - not performing any tranformation!
par(mfrow = c(2,2))
plot(sort(dfAuto$horsepower), main = "horsepower (ASIS)")
plot(log(sort(dfAuto$horsepower)), main = "log(horsepower)")
plot(sqrt(sort(dfAuto$horsepower)), main = "sqrt(horsepower)")
plot(sort(dfAuto$horsepower)^2, main = "horsepower ^ 2")
par(mfrow = c(1,1))
names(dfAuto)
plot(sort(dfAuto$weight))
# Checking on weight
plot(sort(dfAuto$weight))
par(mfrow = c(2,2))
plot(sort(dfAuto$weight), main = "Weight (ASIS)")
plot(log(sort(dfAuto$weight)), main = "log(Weight)")
plot(sqrt(sort(dfAuto$weight)), main = "sqrt(Weight)")
plot(sort(dfAuto$weight)^2, main = "Weight^2")
# Checking on acceleration
plot(sort(dfAuto$acceleration))
par(mfrow = c(2,2))
plot(sort(dfAuto$acceleration), main = "acceleration (ASIS)")
plot(log(sort(dfAuto$acceleration)), main = "log(acceleration)")
plot(sqrt(sort(dfAuto$acceleration)), main = "sqrt(acceleration)")
plot(sort(dfAuto$acceleration)^2, main = "acceleration^2")
dfCarSeats = Carseats
str(dfCarSeats)
?Carseats
dfCarSeats = Carseats
str(dfCarSeats)
summary(dfCarSeats)
lm10MLR1 = lm(formula = Sales ~ Price+Urban+US, data = dfCarSeats)
summary(lm10MLR1)
ificantluse()
lm10MLR2 = update(object = lm10MLR1, formula. = Sales ~ Price+US )
lm10MLR2 = update(object = lm10MLR1, formula. = Sales ~ Price+US )
summary(lm10MLR1)
summary(lm10MLR2)
sqrt(mean(lm10MLR2$residuals^2))
mean(lm10MLR2$fitted.values)
sqrt(mean(lm10MLR2$residuals^2))/mean(lm10MLR2$fitted.values)
predict(object = lm10MLR2, interval = "confidence")
confint(object = lm10MLR2, level = 0.90)
confint(object = lm10MLR2, level = 0.95)
par(mfrow = c(2,2))
plot(lm10MLR2)
par(mfrow = c(2,2))
plot(lm10MLR2)
par(mfrow = c(1,1))
leverage.plot(lm10MLR2)
leveragePlot(model = lm10MLR2)
par(mfrow = c(2,2))
plot(lm10MLR2)
par(mfrow = c(2,2))
plot(lm10MLR2)
par(mfrow = c(1,1))
summary(lm10MLR2$fitted.values)
par(mfrow = c(3,2))
plot(lm10MLR2, which = 1:6)
par(mfrow = c(1,1))
summary(lm10MLR2$residuals)
hist(lm10MLR2$residuals)
hist(lm10MLR2$fitted.values)
par(mfrow = c(3,2))
plot(lm10MLR2, which = 1:6)
par(mfrow = c(1,1))
hist(lm10MLR2$residuals)
mean(lm10MLR2$residuals)
sd(lm10MLR2$residuals)
sd(lm10MLR2$residuals)*1.96
par(mfrow = c(3,2))
plot(lm10MLR2, which = 1:6)
par(mfrow = c(1,1))
# Drawing default 4 plots
par(mfrow = c(2,2))
plot(lm10MLR2)
par(mfrow = c(1,1))
rstudent(lm10MLR2)
summary(lm10MLR2$residuals)
summary(rstudent(lm10MLR2))
round(summary(rstudent(lm10MLR2)), 4)
cooks.distance(lm10MLR2)
summary(cooks.distance(lm10MLR2))
round(summary(cooks.distance(lm10MLR2)), 4)
?leveragePlot
leveragePlot(lm10MLR2)
leveragePlots(lm10MLR2)
influence(lm10MLR2)
summary(influence(lm10MLR2))
outlierTest(lm10MLR2)
outlierTest(model = lm10MLR2)
lm10MLR2
leveragePlots(model = lm10MLR2, ask = TRUE)
influence.measures(model = lm10MLR2)
influencePlot(model = lm10MLR2)
influencePlot(model = lm10MLR2)
par(mfrow = c(1,1))
influencePlot(model = lm10MLR2)
set.seed (1)
x=rnorm (100)
y=2*x+rnorm (100)
lm11SLR1 = lm(y~x + 0)
summary(lm11SLR1)
plot(x,y)
abline(lm11SLR1)
hist(lm11SLR1$residuals)
hist(lm11SLR1$fitted.values)
summary(lm11SLR1)
# Performing regression without Intercept
lm11SLR2 = lm(x~y + 0)
summary(lm11SLR1)
lm11SLR3 = lm(x~y)
lm11SLR4 = lm(y~x)
anova(lm11SLR3, lm11SLR4)
lm11SLR3 = lm(x~y)
lm11SLR4 = lm(y~x)
summary(lm11SLR3)
summary(lm11SLR4)
